{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retinal Disease Detection and Classification\n",
    "\n",
    "## By: Jalen Wu, Yechan Na, Jonathan Zhang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Project Description:__\n",
    "\n",
    "The goal of this project is to develop a machine learning model capable of detecting retinal diseases by analyzing fundus images of the eye. Using computer vision and deep learning techniques, the model assists in early detection and diagnosis of retinal disease(s).\n",
    "\n",
    "__Applications and Impact:__\n",
    "\n",
    "This project could be used for clinical screening to help ophthalmologists identify diseases and improve efficiency on identifying these diseases. This automated detection system has the potential to make medical imaging diagnostics more accessible.\n",
    "\n",
    "__What we hope to achieve:__\n",
    "\n",
    "We hope to build a model that takes in images of the eye and accurately predicts whether an individualâ€™s eyes are healthy or showing signs of disease. To quantify the effectiveness of our model, we will be measuring metrics such as F1 score, precision, recall, loss, and accuracy of our models and graphing them as a function of how many epochs we run on our training data (all of these scores should increase in subsequent epochs).\n",
    "\n",
    "- __Dataset__: https://www.kaggle.com/datasets/andrewmvd/retinal-disease-classification/data\n",
    "- __References__: \n",
    "    - https://www.mdpi.com/2306-5729/6/2/14\n",
    "    - https://jamanetwork.com/journals/jama/fullarticle/2588763\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "from torch import tensor\n",
    "from PIL import Image\n",
    "from torch import flatten\n",
    "from nltk.metrics.scores import (precision, recall, f_measure, accuracy)\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Retinal Disease dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/jalenwu/.cache/kagglehub/datasets/andrewmvd/retinal-disease-classification/versions/1\n"
     ]
    }
   ],
   "source": [
    "# Import Dataset\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"andrewmvd/retinal-disease-classification\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the device to run our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a Transform function to be applied to all images in our dataset and verifying success of dataset download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jalenwu/.cache/kagglehub/datasets/andrewmvd/retinal-disease-classification/versions/1\n",
      "Directory exists and its contents are:\n",
      "['.DS_Store', 'Evaluation_Set', 'Training_Set', 'Test_Set']\n"
     ]
    }
   ],
   "source": [
    "# X = 1424 x 2144 x 3 : h x w x colors\n",
    "# y = label\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((int(1424 / 8), int(2144 / 8))), # Standardize image dimensions\n",
    "    transforms.ToTensor(),          # Convert images to PyTorch tensors\n",
    "])\n",
    "\n",
    "base_directory = path  \n",
    "print(base_directory)\n",
    "\n",
    "if os.path.exists(base_directory):\n",
    "    print(\"Directory exists and its contents are:\")\n",
    "    print(os.listdir(base_directory))\n",
    "else:\n",
    "    print(\"Directory does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining constants for a single point of control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants (paths that will be accessed later in the project)\n",
    "BASE_DIRECTORY = path\n",
    "TRAINING_DIRECTORY = os.path.join(BASE_DIRECTORY, 'Training_Set', 'Training_Set', 'Training')\n",
    "TRAINING_LABELS = os.path.join(BASE_DIRECTORY, 'Training_Set', 'Training_Set', 'RFMiD_Training_Labels.csv')\n",
    "TESTING_DIRECTORY = os.path.join(BASE_DIRECTORY, 'Test_Set', 'Test_Set', 'Test')\n",
    "TESTING_LABELS = os.path.join(BASE_DIRECTORY, 'Test_Set', 'Test_Set', 'RFMiD_Testing_Labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialized training and testing labels for images in Pandas DataFrames to add support for data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delegated the dataframe creation outside of the dataset class\n",
    "training_labels_df = pd.read_csv(TRAINING_LABELS)\n",
    "testing_labels_df = pd.read_csv(TESTING_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Disease_Risk</th>\n",
       "      <th>DR</th>\n",
       "      <th>ARMD</th>\n",
       "      <th>MH</th>\n",
       "      <th>DN</th>\n",
       "      <th>MYA</th>\n",
       "      <th>BRVO</th>\n",
       "      <th>TSLN</th>\n",
       "      <th>ERM</th>\n",
       "      <th>...</th>\n",
       "      <th>CME</th>\n",
       "      <th>PTCR</th>\n",
       "      <th>CF</th>\n",
       "      <th>VH</th>\n",
       "      <th>MCA</th>\n",
       "      <th>VS</th>\n",
       "      <th>BRAO</th>\n",
       "      <th>PLQ</th>\n",
       "      <th>HPED</th>\n",
       "      <th>CL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Disease_Risk  DR  ARMD  MH  DN  MYA  BRVO  TSLN  ERM  ...  CME  PTCR  \\\n",
       "0   1             1   1     0   0   0    0     0     0    0  ...    0     0   \n",
       "1   2             1   1     0   0   0    0     0     0    0  ...    0     0   \n",
       "2   3             1   1     0   0   0    0     0     0    0  ...    0     0   \n",
       "3   4             1   0     0   1   0    0     0     0    0  ...    0     0   \n",
       "4   5             1   1     0   0   0    0     0     0    0  ...    0     0   \n",
       "\n",
       "   CF  VH  MCA  VS  BRAO  PLQ  HPED  CL  \n",
       "0   0   0    0   0     0    0     0   0  \n",
       "1   0   0    0   0     0    0     0   0  \n",
       "2   0   0    0   0     0    0     0   0  \n",
       "3   0   0    0   0     0    0     0   0  \n",
       "4   0   0    0   0     0    0     0   0  \n",
       "\n",
       "[5 rows x 47 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_labels_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Disease_Risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Disease_Risk\n",
       "0             1\n",
       "1             1\n",
       "2             1\n",
       "3             1\n",
       "4             1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_labels_df = training_labels_df.filter(['Disease_Risk'])\n",
    "testing_labels_df = testing_labels_df.filter(['Disease_Risk'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created a custom Dataset class that extends the abstract Dataset class provided by PyTorch. \n",
    "- This step was necessary as the built-in Dataset classes offered by PyTorch were not compatible with the local file structure of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Supports structure of given dataset (images in child folder and labels in csv format).\n",
    "    \"\"\"\n",
    "    def __init__(self, label_csv_file, image_directory, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            label_csv_file (DataFrame): Dataframe of the CSV label file.\n",
    "            image_directory (str): Directory with eye images.\n",
    "            transform (callable, optional): transform function to be applied to each image.\n",
    "        \"\"\"\n",
    "        self.label_csv_file = label_csv_file\n",
    "        self.image_directory = image_directory\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.label_csv_file)    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index: the index of the image/label pair we want to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            image_and_label (dict): A dictionary containing the image and its corresponding label at the requested index.\n",
    "        \"\"\"\n",
    "\n",
    "        # The images are PNG and one-indexed (1.png, 2.png, 3.png, ...)\n",
    "        image_path = os.path.join(self.image_directory, str(index + 1) +'.png')\n",
    "        \n",
    "        image_label = self.label_csv_file.loc[index]\n",
    "        image_label = image_label.to_numpy()\n",
    "        image_label = tensor(image_label)\n",
    "        \n",
    "        # Loads image at the given path.\n",
    "        current_image = io.imread(image_path)\n",
    "        \n",
    "        if current_image is None or current_image.size == 0:\n",
    "            raise ValueError(f\"Failed to load image: {image_path}\")\n",
    "       \n",
    "        # Convert to PIL Image\n",
    "        current_image = Image.fromarray(current_image)\n",
    "        \n",
    "        # If transform function passed in, apply transform to image\n",
    "        if self.transform:\n",
    "            transformed_image = self.transform(current_image)\n",
    "            return {'image': transformed_image, 'label': image_label}\n",
    "        \n",
    "        return {'image': current_image,'label': image_label}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialized Training dataset and loader for later use in a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing dataset and loader.\n",
    "train_dataset = MultiClassDataset(label_csv_file=training_labels_df, image_directory=TRAINING_DIRECTORY, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64)\n",
    "\n",
    "# Visualize loader\n",
    "res = next(iter(train_loader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifying correct initialization of items contained in data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 178, 268])\n",
      "torch.Size([64, 1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(res['image'].shape)\n",
    "print(res['label'].shape)\n",
    "\n",
    "print(res['label'][0])\n",
    "print(res['label'][1])\n",
    "print(res['label'][2])\n",
    "print(res['label'][3])\n",
    "print(res['image'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 178, 268])\n"
     ]
    }
   ],
   "source": [
    "# image tensor format: [batch_size, channels, height, width]\n",
    "print(res['image'][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construction of our Convolutional Neural Network\n",
    "- 3 groupings of convolutional layer, ReLU activation layer, and pooling layer.\n",
    "- Flattening to transfer data to fully connected layers.\n",
    "- 4 fully connected layers, each followed by a ReLU activation layer.\n",
    "- 1 output layer, utilizing sigmoid activation to classify the output of the network.\n",
    "\n",
    "The forward function is called every time a set of data is passed through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Network\n",
    "class Conv_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.conv3 = nn.Conv2d(16, 16, 5)\n",
    "        self.fc1 = nn.Linear(8640, 256)\n",
    "        self.fc2 = nn.Linear(256, 120)\n",
    "        self.fc3 = nn.Linear(120, 84)\n",
    "        self.fc4 = nn.Linear(84, 23)\n",
    "        self.fc5 = nn.Linear(23, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.sigmoid(self.fc5(x))\n",
    "        return x\n",
    "\n",
    "conv_model = Conv_NN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of our training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    \"\"\"\n",
    "    Trains the model for one epoch.\n",
    "    \n",
    "    Args:\n",
    "        dataloader (DataLoader): DataLoader object with training data\n",
    "        model (nn.Module): Model to be trained\n",
    "        loss_fn (nn.Module): Loss function to be used\n",
    "        optimizer (torch.optim): Optimizer to be used\n",
    "    \n",
    "    Returns:\n",
    "        float: Average loss for the current epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for i, data in tqdm(enumerate(dataloader)):\n",
    "        # get the inputs; data is a dict of [inputs, labels]\n",
    "        inputs = data['image']\n",
    "        labels = data['label']\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels.type(torch.FloatTensor))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss/size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of test dataset and loader for testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MultiClassDataset(label_csv_file=testing_labels_df, image_directory=TESTING_DIRECTORY, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to determine precision, recall, f1, and accuracy metrics when given a list of predictions and a list of true values (common measures of model effectiveness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PRFA(predictions, answers):\n",
    "    \"\"\"\n",
    "    Returns precision, recall, f1, and accuracy of the predictions.\n",
    "    \n",
    "    Args:\n",
    "        predictions (list): List of predicted labels\n",
    "        answers (list): List of actual labels\n",
    "    \"\"\"\n",
    "    pred_indices = [x for x in range(len(predictions)) if predictions[x] == 1]\n",
    "    actual_indices = [y for y in range(len(answers)) if answers[y] == 1]\n",
    "\n",
    "    temp_precision = precision(set(pred_indices), set(actual_indices)) # actual labels vs. predicted labels\n",
    "    temp_recall = recall(set(pred_indices), set(actual_indices))\n",
    "    temp_f1 = f_measure(set(pred_indices), set(actual_indices))\n",
    "    temp_accuracy = accuracy(answers, predictions)\n",
    "    return (temp_precision, temp_recall, temp_f1, temp_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_values = []\n",
    "\n",
    "def get_preds(dataloader, model):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for i, data in tqdm(enumerate(dataloader)):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs = data['image']\n",
    "            l = data['label']\n",
    "            \n",
    "            labels.append(l)\n",
    "            preds.append(model(inputs))\n",
    "    return torch.cat(preds), torch.cat(labels)\n",
    "\n",
    "\n",
    "def get_clamped_preds(preds, t):\n",
    "    \"\"\"\n",
    "    Takes a dataloader for the test data and model\n",
    "    Returns the Precision, Recall, F1 Score, and Accuracy of the model as a tuple\n",
    "    \"\"\"\n",
    "    \n",
    "    new_preds = torch.tensor(np.zeros([len(preds)], dtype=int))\n",
    "    \n",
    "    # Goes through all images in preds\n",
    "    for img_in_batch in range(len(preds)): \n",
    "        # Goes through all disease labels in the image\n",
    "        # Clamp the values to 0 or 1\n",
    "        if preds[img_in_batch] > t:\n",
    "            new_preds[img_in_batch] = 1\n",
    "        else:\n",
    "            new_preds[img_in_batch] = 0\n",
    "    return new_preds\n",
    "\n",
    "\n",
    "def test(new_preds, labels):\n",
    "    size = len(new_preds)\n",
    "    correct = 0\n",
    "    for i in range(len(new_preds)):\n",
    "        if new_preds[i] == labels[i]:\n",
    "            correct += 1\n",
    "\n",
    "    accuracy = correct / size\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    return PRFA(new_preds, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer: Stochastic Gradient Descent with Momentum\n",
    "- learning rate set to 0.001 \n",
    "- momentum (beta) set to 0.9\n",
    "\n",
    "$$\n",
    "w_{t+1} = w_t - V_t\n",
    "$$\n",
    "\n",
    "$$\n",
    "V_t = V_{t-1} + (1 - \\beta) \\frac{\\partial L}{\\partial w_t} \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "# Changed learning rate from 0.001 to 0.01\n",
    "optimizer = optim.SGD(conv_model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Neural Network\n",
    "- Maximum number of epochs: 40\n",
    "- Each epoch runs train, then test\n",
    "- PRFA scores are saved and written to .txt file for later use or reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [05:47, 11.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolutional Model Loss:  0.011058783996850252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [01:55, 11.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.309375\n",
      "prfa: \n",
      "0.12648221343873517 1.0 0.22456140350877193 tensor([0.3094])\n",
      "Epoch 2\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [05:51, 11.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolutional Model Loss:  0.010581581170360248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [01:56, 11.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3765625\n",
      "prfa: \n",
      "0.21541501976284586 0.9819819819819819 0.35332252836304695 tensor([0.3766])\n",
      "Epoch 3\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [07:41, 15.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolutional Model Loss:  0.010157201625406741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [02:30, 15.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.4078125\n",
      "prfa: \n",
      "0.25889328063241107 0.9703703703703703 0.40873634945397813 tensor([0.4078])\n",
      "Epoch 4\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [09:28, 18.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolutional Model Loss:  0.009806633461266756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [02:02, 12.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.4\n",
      "prfa: \n",
      "0.2509881422924901 0.9621212121212122 0.3981191222570533 tensor([0.4000])\n",
      "Epoch 5\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [05:40, 11.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolutional Model Loss:  0.00951043733706077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [01:53, 11.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.4359375\n",
      "prfa: \n",
      "0.30434782608695654 0.9447852760736196 0.46038863976083705 tensor([0.4359])\n",
      "Epoch 6\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [05:41, 11.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolutional Model Loss:  0.009257810718069474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [02:12, 13.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.4125\n",
      "prfa: \n",
      "0.2727272727272727 0.9452054794520548 0.42331288343558277 tensor([0.4125])\n",
      "Epoch 7\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [06:01, 12.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolutional Model Loss:  0.00904250293970108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [01:59, 11.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3875\n",
      "prfa: \n",
      "0.24110671936758893 0.9384615384615385 0.3836477987421384 tensor([0.3875])\n",
      "Epoch 8\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [05:36, 11.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolutional Model Loss:  0.008856751226509611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [02:01, 12.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.315625\n",
      "prfa: \n",
      "0.1383399209486166 0.9722222222222222 0.24221453287197228 tensor([0.3156])\n",
      "Epoch 9\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [05:53, 11.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolutional Model Loss:  0.008695403983195623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [01:55, 11.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.2984375\n",
      "prfa: \n",
      "0.116600790513834 0.9672131147540983 0.20811287477954143 tensor([0.2984])\n",
      "Epoch 10\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [05:39, 11.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolutional Model Loss:  0.008550565581147869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [01:54, 11.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3125\n",
      "prfa: \n",
      "0.13438735177865613 0.9714285714285714 0.2361111111111111 tensor([0.3125])\n",
      "Epoch 11\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [05:47, 11.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolutional Model Loss:  0.008413354633376002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [02:05, 12.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3578125\n",
      "prfa: \n",
      "0.1956521739130435 0.9611650485436893 0.3251231527093596 tensor([0.3578])\n",
      "Epoch 12\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [06:06, 12.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolutional Model Loss:  0.008279450656846165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [01:54, 11.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.4015625\n",
      "prfa: \n",
      "0.25889328063241107 0.9424460431654677 0.40620155038759687 tensor([0.4016])\n",
      "Epoch 13\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [06:01, 12.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolutional Model Loss:  0.008156304030368726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [02:04, 12.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.4203125\n",
      "prfa: \n",
      "0.2865612648221344 0.9354838709677419 0.43872919818456885 tensor([0.4203])\n",
      "Epoch 14\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [06:01, 12.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolutional Model Loss:  0.008060840998465817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [02:02, 12.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.4359375\n",
      "prfa: \n",
      "0.3102766798418972 0.9289940828402367 0.46518518518518515 tensor([0.4359])\n",
      "Epoch 15\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [06:33, 13.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolutional Model Loss:  0.008001403541614611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [02:03, 12.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.4484375\n",
      "prfa: \n",
      "0.3300395256916996 0.9226519337016574 0.48617176128093154 tensor([0.4484])\n",
      "Epoch 16\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [07:39, 15.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolutional Model Loss:  0.007970909013723333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [02:06, 12.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.453125\n",
      "prfa: \n",
      "0.3359683794466403 0.9239130434782609 0.4927536231884058 tensor([0.4531])\n",
      "Epoch 17\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [06:30, 13.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolutional Model Loss:  0.007959731590623657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [02:29, 14.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.4484375\n",
      "prfa: \n",
      "0.3221343873517787 0.9421965317919075 0.48011782032400596 tensor([0.4484])\n",
      "Epoch 18\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [07:44, 15.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolutional Model Loss:  0.007956184741730491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [01:49, 10.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.4421875\n",
      "prfa: \n",
      "0.3142292490118577 0.9408284023668639 0.4711111111111111 tensor([0.4422])\n",
      "Epoch 19\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [05:36, 11.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolutional Model Loss:  0.007953492101902764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [02:14, 13.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.440625\n",
      "prfa: \n",
      "0.308300395256917 0.9512195121951219 0.4656716417910448 tensor([0.4406])\n",
      "Epoch 20\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [06:01, 12.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolutional Model Loss:  0.007950710598379374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [01:57, 11.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.4390625\n",
      "prfa: \n",
      "0.30632411067193677 0.950920245398773 0.46337817638266066 tensor([0.4391])\n",
      "Epoch 21\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [05:55, 11.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolutional Model Loss:  0.007947629659126203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [02:00, 12.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.4375\n",
      "prfa: \n",
      "0.30434782608695654 0.9506172839506173 0.4610778443113773 tensor([0.4375])\n",
      "Epoch 22\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [05:53, 11.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolutional Model Loss:  0.007944561339293917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [02:01, 12.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.4375\n",
      "prfa: \n",
      "0.30434782608695654 0.9506172839506173 0.4610778443113773 tensor([0.4375])\n",
      "Epoch 23\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [06:30, 13.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolutional Model Loss:  0.007941585732623935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [02:02, 12.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.4375\n",
      "prfa: \n",
      "0.30434782608695654 0.9506172839506173 0.4610778443113773 tensor([0.4375])\n",
      "Epoch 24\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [05:59, 11.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolutional Model Loss:  0.007938480641071995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [02:11, 13.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.4359375\n",
      "prfa: \n",
      "0.30237154150197626 0.9503105590062112 0.4587706146926536 tensor([0.4359])\n",
      "Epoch 25\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [06:45, 13.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolutional Model Loss:  0.007935411995276809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [01:51, 11.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.434375\n",
      "prfa: \n",
      "0.2984189723320158 0.9556962025316456 0.45481927710843373 tensor([0.4344])\n",
      "Epoch 26\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [06:11, 12.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolutional Model Loss:  0.007932313081497947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [01:59, 11.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.434375\n",
      "prfa: \n",
      "0.2964426877470356 0.9615384615384616 0.45317220543806647 tensor([0.4344])\n",
      "Epoch 27\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [19:47, 39.57s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolutional Model Loss:  0.007929161237552763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [01:52, 11.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.4296875\n",
      "prfa: \n",
      "0.29051383399209485 0.9607843137254902 0.4461305007587253 tensor([0.4297])\n",
      "Epoch 28\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [06:22, 12.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolutional Model Loss:  0.007925776314611237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [01:53, 11.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.43125\n",
      "prfa: \n",
      "0.29051383399209485 0.9671052631578947 0.44680851063829785 tensor([0.4313])\n",
      "Epoch 29\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [06:00, 12.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolutional Model Loss:  0.007922232119987408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [01:50, 11.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.4328125\n",
      "prfa: \n",
      "0.2885375494071146 0.9798657718120806 0.44580152671755724 tensor([0.4328])\n",
      "Epoch 30\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [06:04, 12.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolutional Model Loss:  0.007918744099636872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [01:53, 11.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.43125\n",
      "prfa: \n",
      "0.2865612648221344 0.9797297297297297 0.44342507645259943 tensor([0.4313])\n",
      "Epoch 31\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27it [06:51, 15.25s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m     conv_loss \u001b[38;5;241m=\u001b[39m train(train_loader, conv_model, loss_fn, optimizer)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvolutional Model Loss: \u001b[39m\u001b[38;5;124m\"\u001b[39m, conv_loss)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m conv_loss \u001b[38;5;241m>\u001b[39m conv_model_loss[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n",
      "Cell \u001b[0;32mIn[55], line 5\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      3\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[1;32m      4\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(dataloader)):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# get the inputs; data is a dict of [inputs, labels]\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      8\u001b[0m     labels \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[50], line 36\u001b[0m, in \u001b[0;36mMultiClassDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     33\u001b[0m image_label \u001b[38;5;241m=\u001b[39m tensor(image_label)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Loads image at the given path.\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m current_image \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mimread(image_path)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_image \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m current_image\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to load image: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/skimage/io/_io.py:53\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(fname, as_gray, plugin, **plugin_args)\u001b[0m\n\u001b[1;32m     50\u001b[0m         plugin \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtifffile\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m file_or_url_context(fname) \u001b[38;5;28;01mas\u001b[39;00m fname:\n\u001b[0;32m---> 53\u001b[0m     img \u001b[38;5;241m=\u001b[39m call_plugin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimread\u001b[39m\u001b[38;5;124m'\u001b[39m, fname, plugin\u001b[38;5;241m=\u001b[39mplugin, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mplugin_args)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(img, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/skimage/io/manage_plugins.py:205\u001b[0m, in \u001b[0;36mcall_plugin\u001b[0;34m(kind, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCould not find the plugin \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mplugin\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkind\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/skimage/io/_plugins/imageio_plugin.py:15\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(imageio_imread)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimread\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(imageio_imread(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/imageio/v2.py:360\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(uri, format, **kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m imopen_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlegacy_mode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m imopen(uri, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mri\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mimopen_args) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m--> 360\u001b[0m     result \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/imageio/plugins/pillow.py:224\u001b[0m, in \u001b[0;36mPillowPlugin.read\u001b[0;34m(self, index, mode, rotate, apply_gamma, writeable_output, pilmode, exifrotate, as_gray)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(index, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m# will raise IO error if index >= number of frames in image\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_image\u001b[38;5;241m.\u001b[39mseek(index)\n\u001b[0;32m--> 224\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_transforms(\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_image, mode, rotate, apply_gamma, writeable_output\n\u001b[1;32m    226\u001b[0m     )\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    228\u001b[0m     iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter(\n\u001b[1;32m    229\u001b[0m         mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[1;32m    230\u001b[0m         rotate\u001b[38;5;241m=\u001b[39mrotate,\n\u001b[1;32m    231\u001b[0m         apply_gamma\u001b[38;5;241m=\u001b[39mapply_gamma,\n\u001b[1;32m    232\u001b[0m         writeable_output\u001b[38;5;241m=\u001b[39mwriteable_output,\n\u001b[1;32m    233\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/imageio/plugins/pillow.py:307\u001b[0m, in \u001b[0;36mPillowPlugin._apply_transforms\u001b[0;34m(self, image, mode, rotate, apply_gamma, writeable_output)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m    304\u001b[0m         \u001b[38;5;66;03m# Let pillow know that it is okay to return 16-bit\u001b[39;00m\n\u001b[1;32m    305\u001b[0m         image\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m=\u001b[39m desired_mode\n\u001b[0;32m--> 307\u001b[0m image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(image)\n\u001b[1;32m    309\u001b[0m meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_image\u001b[38;5;241m.\u001b[39mtell(), exclude_applied\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rotate \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOrientation\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m meta:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/PIL/Image.py:701\u001b[0m, in \u001b[0;36mImage.__array_interface__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m         new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtobytes(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 701\u001b[0m         new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, (\u001b[38;5;167;01mMemoryError\u001b[39;00m, \u001b[38;5;167;01mRecursionError\u001b[39;00m)):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/PIL/Image.py:758\u001b[0m, in \u001b[0;36mImage.tobytes\u001b[0;34m(self, encoder_name, *args)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m args \u001b[38;5;241m==\u001b[39m ():\n\u001b[1;32m    756\u001b[0m     args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\n\u001b[0;32m--> 758\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m    760\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheight \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    761\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/PIL/ImageFile.py:269\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[1;32m    268\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 269\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(b)\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 40\n",
    "conv_model_loss = []\n",
    "p_scores = []\n",
    "r_scores = []\n",
    "f_scores = []\n",
    "a_scores = []\n",
    "\n",
    "#conv_model = Conv_NN()\n",
    "#conv_model.load_state_dict(torch.load(\"5th_conv_model.pth\", weights_only=True))\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    conv_loss = train(train_loader, conv_model, loss_fn, optimizer)\n",
    "    print(\"Convolutional Model Loss: \", conv_loss)\n",
    "    \n",
    "    # Early stop if loss has increased.\n",
    "    if t > 3 and conv_loss > conv_model_loss[-1]:\n",
    "        break\n",
    "        \n",
    "    preds, labels = get_preds(test_loader, conv_model)\n",
    "    avg = (max(preds) + min(preds)) / 2\n",
    "    # print(avg)\n",
    "    new_preds = get_clamped_preds(torch.flatten(preds), avg)\n",
    "    # print(new_preds[0])\n",
    "    p, r, f, a = test(new_preds, labels)\n",
    "    \n",
    "    print(\"prfa: \")\n",
    "    print(p, r, f, a)\n",
    "    p_scores.append(p)\n",
    "    r_scores.append(r)\n",
    "    f_scores.append(f)\n",
    "    a_scores.append(a)\n",
    "    \n",
    "     # if the accuracy has decreased, early stop\n",
    "    # if len(a_scores) > 1 and a_scores[-1] < a_scores[-2]:\n",
    "    #     break\n",
    "    \n",
    "    conv_model_loss.append(conv_loss)\n",
    "    torch.save(conv_model.state_dict(), \"binary_model.pth\")\n",
    "\n",
    "\n",
    "    file = open(\"Convolutional_model_scores.txt\", \"w\", encoding=\"utf8\")\n",
    "    file.write(f\"losses: {conv_model_loss} \\n\")\n",
    "    file.write(f\"precisions: {p_scores} \\n\")\n",
    "    file.write(f\"recalls: {r_scores} \\n\")\n",
    "    file.write(f\"f1 scores: {f_scores} \\n\")\n",
    "    file.write(f\"accuracies: {a_scores} \\n\")\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [02:05, 12.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6546])\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "conv_model = Conv_NN()\n",
    "conv_model.load_state_dict(torch.load(\"binary_model.pth\", weights_only=True))\n",
    "\n",
    "preds, labels = get_preds(test_loader, conv_model)\n",
    "avg = (max(preds) + min(preds)) / 2\n",
    "print(avg)\n",
    "new_preds = get_clamped_preds(torch.flatten(preds), avg)\n",
    "print(new_preds[0])\n",
    "print(new_preds[1])\n",
    "print(new_preds[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions \n",
      "--------------------\n",
      "tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 1])\n",
      "Actual \n",
      "--------------------\n",
      "tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(\"Predictions \\n--------------------\")\n",
    "print(new_preds[-100:])\n",
    "# pred_one = 0\n",
    "# pred_zero = 0\n",
    "# for i in new_preds:\n",
    "#     if i == 1:\n",
    "#         pred_one += 1\n",
    "#     else:\n",
    "#         pred_zero += 1\n",
    "\n",
    "# print(\"Predicted 1: \", pred_one)\n",
    "# print(\"Predicted 0: \", pred_zero)\n",
    "        \n",
    "print(\"Actual \\n--------------------\")\n",
    "print(labels[-100:].flatten())\n",
    "# actual_one = 0\n",
    "# actual_zero = 0\n",
    "# for i in labels:\n",
    "#     if i == 1:\n",
    "#         actual_one += 1\n",
    "#     else:\n",
    "#         actual_zero += 1\n",
    "\n",
    "# print(\"Actual 1: \", actual_one)\n",
    "# print(\"Actual 0: \", actual_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_labels[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "- Plot loss/accuracy (y) with number of epochs ran (x)\n",
    "- Recall/Precision/F1 score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model = Conv_NN()\n",
    "conv_model.load_state_dict(torch.load(\"5th_conv_model.pth\", weights_only=True))\n",
    "\n",
    "test(test_loader, conv_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def graph_loss(x, y, metric):\n",
    "\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel(\"epoch #\")\n",
    "    plt.ylabel(f\"{metric} score per epoch\")\n",
    "    plt.title(f\"{metric} Scores\")\n",
    "    plt.savefig(f\"{metric}_plot.png\")\n",
    "    plt.clf()\n",
    "\n",
    "graph_loss(range(len(conv_model_loss)), conv_model_loss, \"Cross Entropy Loss\")\n",
    "graph_loss(range(len(p_scores)), p_scores, \"Precision\")\n",
    "graph_loss(range(len(r_scores)), r_scores, \"Recall\")\n",
    "graph_loss(range(len(f_scores)), f_scores, \"F1\")\n",
    "graph_loss(range(len(a_scores)), a_scores, \"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First vs Final Scores:\n",
      "Cross Entropy Loss: 0.011058783996850252 0.007918744099636872\n",
      "Precision: 0.12648221343873517 0.2865612648221344\n",
      "Recall: 1.0 0.9797297297297297\n",
      "F1: 0.22456140350877193 0.44342507645259943\n",
      "Accuracy: 0.30937498807907104 0.4312500059604645\n"
     ]
    }
   ],
   "source": [
    "print(\"First vs Final Scores:\")\n",
    "print(\"Cross Entropy Loss:\", conv_model_loss[0], conv_model_loss[-1])\n",
    "print(\"Precision:\", p_scores[0], p_scores[-1])\n",
    "print(\"Recall:\", r_scores[0], r_scores[-1])\n",
    "print(\"F1:\", f_scores[0], f_scores[-1])\n",
    "print(\"Accuracy:\", float(a_scores[0]), float(a_scores[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "imshow(images[0])\n",
    "# print labels\n",
    "print(classes[labels[0]], images[0].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
