{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retinal Disease Detection and Classification\n",
    "\n",
    "## By: Jalen Wu, Yechan Na, Jonathan Zhang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Project Description:__\n",
    "\n",
    "The goal of this project is to develop a machine learning model capable of detecting retinal diseases by analyzing fundus images of the eye. Using computer vision and deep learning techniques, the model assists in early detection and diagnosis of retinal disease(s).\n",
    "\n",
    "__Applications and Impact:__\n",
    "\n",
    "This project could be used for clinical screening to help ophthalmologists identify diseases and improve efficiency on identifying these diseases. This automated detection system has the potential to make medical imaging diagnostics more accessible.\n",
    "\n",
    "__What we hope to achieve:__\n",
    "\n",
    "We hope to build a model that takes in images of the eye and accurately predicts whether an individualâ€™s eyes are healthy or showing signs of disease. To quantify the effectiveness of our model, we will be measuring metrics such as F1 score, precision, recall, loss, and accuracy of our models and graphing them as a function of how many epochs we run on our training data (all of these scores should increase in subsequent epochs).\n",
    "\n",
    "- __Dataset__: https://www.kaggle.com/datasets/andrewmvd/retinal-disease-classification/data\n",
    "- __References__: \n",
    "    - https://www.mdpi.com/2306-5729/6/2/14\n",
    "    - https://jamanetwork.com/journals/jama/fullarticle/2588763\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "from torch import tensor\n",
    "from PIL import Image\n",
    "from torch import flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\wendy\\.cache\\kagglehub\\datasets\\andrewmvd\\retinal-disease-classification\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "# Import Dataset\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"andrewmvd/retinal-disease-classification\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wendy\\.cache\\kagglehub\\datasets\\andrewmvd\\retinal-disease-classification\\versions\\1\n",
      "Directory exists and its contents are:\n",
      "['Evaluation_Set', 'Test_Set', 'Training_Set']\n"
     ]
    }
   ],
   "source": [
    "# X = 1424 x 2144 x 3 : h x w x colors\n",
    "# y = label\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((int(1424 / 8), int(2144 / 8))), # Standardize image dimensions to 1444 x 2144\n",
    "    transforms.ToTensor(),          # Convert images to PyTorch tensors\n",
    "    # transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "base_directory = path  \n",
    "print(base_directory)\n",
    "\n",
    "if os.path.exists(base_directory):\n",
    "    print(\"Directory exists and its contents are:\")\n",
    "    print(os.listdir(base_directory))\n",
    "else:\n",
    "    print(\"Directory does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants (paths that will be accessed later in the project)\n",
    "BASE_DIRECTORY = path\n",
    "TRAINING_DIRECTORY = os.path.join(BASE_DIRECTORY, 'Training_Set', 'Training_Set', 'Training')\n",
    "TRAINING_LABELS = os.path.join(BASE_DIRECTORY, 'Training_Set', 'Training_Set', 'RFMiD_Training_Labels.csv')\n",
    "TESTING_DIRECTORY = os.path.join(BASE_DIRECTORY, 'Test_Set', 'Test_Set', 'Test')\n",
    "TESTING_LABELS = os.path.join(BASE_DIRECTORY, 'Test_Set', 'Test_Set', 'RFMiD_Testing_Labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Supports structure of given dataset (images in child folder and labels in csv format).\n",
    "    \"\"\"\n",
    "    def __init__(self, label_csv_file, image_directory, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            label_csv_file (str): Path to the CSV label file.\n",
    "            image_directory (str): Directory with eye images.\n",
    "            transform (callable, optional): transform function to be applied to each image.\n",
    "        \"\"\"\n",
    "        self.label_csv_file = pd.read_csv(label_csv_file)\n",
    "        self.image_directory = image_directory\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.label_csv_file)    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index: the index of the image/label pair we want to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            image_and_label (dict): A dictionary containing the image and its corresponding label at the requested index.\n",
    "        \"\"\"\n",
    "\n",
    "        # The images are PNG and one-indexed (1.png, 2.png, 3.png, ...)\n",
    "        image_path = os.path.join(self.image_directory, str(index + 1) +'.png')\n",
    "        \n",
    "        image_label = self.label_csv_file.loc[index]\n",
    "        image_label = image_label.to_numpy()\n",
    "        image_label = image_label[1:]\n",
    "        image_label = tensor(image_label)\n",
    "        \n",
    "        # Loads image at the given path.\n",
    "        current_image = io.imread(image_path)\n",
    "        \n",
    "        if current_image is None or current_image.size == 0:\n",
    "            raise ValueError(f\"Failed to load image: {image_path}\")\n",
    "       \n",
    "        # Convert to PIL Image\n",
    "        current_image = Image.fromarray(current_image)\n",
    "        \n",
    "        # If transform function passed in, apply transform to image\n",
    "        if self.transform:\n",
    "            transformed_image = self.transform(current_image)\n",
    "            return {'image': transformed_image, 'label': image_label}\n",
    "        \n",
    "        return {'image': current_image,'label': image_label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 178, 268])\n",
      "torch.Size([64, 46])\n",
      "tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Initializing dataset and loader.\n",
    "train_dataset = MultiClassDataset(label_csv_file=TRAINING_LABELS, image_directory=TRAINING_DIRECTORY, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64)\n",
    "\n",
    "# Visualize loader\n",
    "res = next(iter(train_loader))\n",
    "print(res['image'].shape)\n",
    "print(res['label'].shape)\n",
    "\n",
    "print(res['label'][0])\n",
    "print(res['image'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 178, 268])\n"
     ]
    }
   ],
   "source": [
    "# image tensor format: [batch_size, channels, height, width]\n",
    "print(res['image'][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Conv_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.conv3 = nn.Conv2d(16, 16, 5)\n",
    "        self.fc1 = nn.Linear(41328, 1200)\n",
    "        self.fc2 = nn.Linear(1200, 840)\n",
    "        self.fc3 = nn.Linear(840, 230)\n",
    "        self.fc4 = nn.Linear(230, 46)\n",
    "        #self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.sigmoid(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "conv_model = Conv_NN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(conv_model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    total_loss = 0.0\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dataloader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs = data['image']\n",
    "        labels = data['label']\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels.type(torch.FloatTensor))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        total_loss += loss.item()\n",
    "        if i % n == n-1:    # print every n mini-batches\n",
    "            print(f'{i + 1:5d} loss: {running_loss / n:.3f}')\n",
    "            running_loss = 0.0\n",
    "    \n",
    "    return total_loss/size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "   20 loss: 7.707\n",
      "conv_loss:  0.11186706485847632\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "   20 loss: 7.707\n",
      "conv_loss:  0.11186706485847632\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "   20 loss: 7.707\n",
      "conv_loss:  0.11186706485847632\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "   20 loss: 7.707\n",
      "conv_loss:  0.11186706485847632\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "   20 loss: 7.707\n",
      "conv_loss:  0.11186706485847632\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "   20 loss: 7.707\n",
      "conv_loss:  0.11186706485847632\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "   20 loss: 7.707\n",
      "conv_loss:  0.11186706485847632\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "   20 loss: 7.707\n",
      "conv_loss:  0.11186706485847632\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "   20 loss: 7.707\n",
      "conv_loss:  0.11186706485847632\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "   20 loss: 7.707\n",
      "conv_loss:  0.11186706485847632\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "   20 loss: 7.707\n",
      "conv_loss:  0.11186706485847632\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "   20 loss: 7.707\n",
      "conv_loss:  0.11186706485847632\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "   20 loss: 7.707\n",
      "conv_loss:  0.11186706485847632\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "   20 loss: 7.707\n",
      "conv_loss:  0.11186706485847632\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "   20 loss: 7.707\n",
      "conv_loss:  0.11186706485847632\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "   20 loss: 7.707\n",
      "conv_loss:  0.11186706485847632\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "   20 loss: 7.707\n",
      "conv_loss:  0.11186706485847632\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "   20 loss: 7.707\n",
      "conv_loss:  0.11186706485847632\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "   20 loss: 7.707\n",
      "conv_loss:  0.11186706485847632\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "   20 loss: 7.707\n",
      "conv_loss:  0.11186706485847632\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "   20 loss: 7.707\n",
      "conv_loss:  0.11186706485847632\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "   20 loss: 7.707\n",
      "conv_loss:  0.11186706485847632\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "   20 loss: 7.707\n",
      "conv_loss:  0.11186706485847632\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "   20 loss: 7.707\n",
      "conv_loss:  0.11186706485847632\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "   20 loss: 7.707\n",
      "conv_loss:  0.11186706485847632\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "   20 loss: 7.707\n",
      "conv_loss:  0.11186706485847632\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "   20 loss: 7.707\n",
      "conv_loss:  0.11186706485847632\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "   20 loss: 7.707\n",
      "conv_loss:  0.11186706485847632\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "   20 loss: 7.707\n",
      "conv_loss:  0.11186706485847632\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "   20 loss: 7.707\n",
      "conv_loss:  0.11186706485847632\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "   20 loss: 7.707\n",
      "conv_loss:  0.11186706485847632\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "   20 loss: 7.707\n",
      "conv_loss:  0.11186706485847632\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "   20 loss: 7.707\n",
      "conv_loss:  0.11186706485847632\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "   20 loss: 7.707\n",
      "conv_loss:  0.11186706485847632\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "   20 loss: 7.707\n",
      "conv_loss:  0.11186706485847632\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "   20 loss: 7.707\n",
      "conv_loss:  0.11186706485847632\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "   20 loss: 7.707\n",
      "conv_loss:  0.11186706485847632\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "   20 loss: 7.707\n",
      "conv_loss:  0.11186706485847632\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "   20 loss: 7.707\n",
      "conv_loss:  0.11186706485847632\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "   20 loss: 7.707\n",
      "conv_loss:  0.11186706485847632\n"
     ]
    }
   ],
   "source": [
    "epochs = 40\n",
    "conv_model_loss = []\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    conv_loss = train(train_loader, conv_model, loss_fn, optimizer)\n",
    "    print(\"conv_loss: \", conv_loss)\n",
    "    \n",
    "    if t > 3 and conv_loss > conv_model_loss[-1]:\n",
    "        break\n",
    "    \n",
    "    conv_model_loss.append(conv_loss)\n",
    "    torch.save(conv_model.state_dict(), \"conv_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "- Plot loss/accuracy (y) with number of epochs ran (x)\n",
    "- Recall/Precision/F1 score "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "conv_model = Conv_NN()\n",
    "conv_model.load_state_dict(torch.load(\"conv_model.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    correct_each = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloader):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs = data['image']\n",
    "            labels = data['label']\n",
    "\n",
    "            preds = model(inputs)\n",
    "            for p_i in range(len(preds)):\n",
    "                for l_i in range(len(preds[p_i])):\n",
    "                    if preds[p_i][l_i] > 0.9:\n",
    "                        preds[p_i][l_i] = 1.0\n",
    "                    else:\n",
    "                        preds[p_i][l_i] = 0.0\n",
    "            # TODO: look at (preds == labels)\n",
    "            for row in (preds == labels):\n",
    "                all_correct = 1\n",
    "                for equal in row:\n",
    "                    if not equal:\n",
    "                        all_correct = 0\n",
    "                        break\n",
    "                correct += all_correct\n",
    "            correct_each += (preds == labels).type(torch.float).sum().item()/len(preds[0])\n",
    "            print(correct, correct_each)\n",
    "    \n",
    "    accuracy = correct/size\n",
    "    accuracy_each = correct_each/size\n",
    "    print(\"accuracy:\", accuracy)\n",
    "    print(\"accuracy_each:\", accuracy_each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MultiClassDataset(label_csv_file=TESTING_LABELS, image_directory=TESTING_DIRECTORY, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolutional Model\n",
      "0 60.58695652173913\n",
      "0 121.28260869565219\n",
      "0 182.1521739130435\n",
      "1 243.23913043478262\n",
      "15 304.5869565217391\n",
      "27 365.9130434782609\n",
      "37 427.1304347826087\n",
      "61 489.0869565217391\n",
      "104 552.1739130434783\n",
      "134 614.4565217391304\n",
      "accuracy: 0.209375\n",
      "accuracy_each: 0.9600883152173912\n"
     ]
    }
   ],
   "source": [
    "print(\"Convolutional Model\")\n",
    "test(test_loader, conv_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.11186706485847632, 0.11186706485847632, 0.11186706485847632, 0.11186706485847632, 0.11186706485847632, 0.11186706485847632, 0.11186706485847632, 0.11186706485847632, 0.11186706485847632, 0.11186706485847632, 0.11186706485847632, 0.11186706485847632, 0.11186706485847632, 0.11186706485847632, 0.11186706485847632, 0.11186706485847632, 0.11186706485847632, 0.11186706485847632, 0.11186706485847632, 0.11186706485847632, 0.11186706485847632, 0.11186706485847632, 0.11186706485847632, 0.11186706485847632, 0.11186706485847632, 0.11186706485847632, 0.11186706485847632, 0.11186706485847632, 0.11186706485847632, 0.11186706485847632, 0.11186706485847632, 0.11186706485847632, 0.11186706485847632, 0.11186706485847632, 0.11186706485847632, 0.11186706485847632, 0.11186706485847632, 0.11186706485847632, 0.11186706485847632, 0.11186706485847632]\n"
     ]
    }
   ],
   "source": [
    "def graph_loss(x, y, graph_name):\n",
    "\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel(\"epoch #\")\n",
    "    plt.ylabel(\"avg loss per epoch\")\n",
    "    plt.title(graph_name)\n",
    "    plt.show()\n",
    "    \n",
    "graph_loss(range(len(conv_model_loss)), conv_model_loss, \"convolutional model\")\n",
    "print(conv_model_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
